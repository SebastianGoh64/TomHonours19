---
title: "Thesis"
author: "Thomas Goh"
date: "06/08/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Importing data & setting class

Originally there was an issue that the final numbers of tweet_id were rounded. This was b/c it was being imported as integer. Used ColClasses to import as character.

Currently having issues with DateTime. No DateTime format, will need to convert to Date format (time is unimportant). Currently imported as character. 

test to cut string down & read as.Date
date_test <- head(raw_tweets$created_at,20)
date_test <- substring(date_test, 1,10)


Corpus transformation will require a dataframe with unique ID at start and text in 2nd column. All other columns will be imported as metadata.

When was the first tweet?
How many tweets each politician?
Plot total tweets over time.

### Summary Statistics

Top 20 'tweeters

`head(tweet_count, 20)`

Can't knit something from the global environment. Best idea is to save table as csv and load csv in RMarkdown. Can possibly also save an image of the workspace?

https://stackoverflow.com/questions/42340928/knit-error-object-not-found
```{r}


knitr::kable(head(tweet_count, 10), format = "markdown")
```

Still working out how to filter the dataset on multiple words. Can filter it on one word fairly easily so far. 

Creating document term matrix created some errors as it said the file would be too big (~25gb). Need to work out whether this is necessary to be doing word counts. Trial with smaller sets of data

### Pre-Processing Text

With the tm package you can remove stopwords, whitespace and punctuation and convert to lower very easily. (for corpus objects).

Removing emoji and URLs have been a bit more complex. When the text is just in a dataframe it's easy to clean using gsub function. (might need to test a  bit more). Theoretically the tm_map function should be able to work with the gsub function but it doesn't seem to actually change the corpus. Might just need to clean it in dataframe before putting into corpus.

I'm reluctant to change the raw_tweets dataframe though. Also don't want to create too many dataframes b/c will lose them easily. 

### Methodology 

Twitter doesn't ban politicians, insteads hides content that might be inflammatory.

Is there a common factor behind deleted accounts/private accounts and not having an account? Need to consider whether there is a bias in the attrition or whether it is random. 

###Current Issues

- Doing word counts (too big to put into document term matrix)
 
- removing URLs in the cleanest way possible. 

## R Markdown - Workshop

Quick commands
* Italics * - *Italics*
** Bold ** - **Bold**
` code ` - `code`
[links](rmarkdown.rstudio.com)
Many more

LateX equations ($)
$E = mc^2 $

Bibliographies [@rmarkdown15]

You can create a list by starting lines with - + or *

*Item A
  +Item a
  *Item aa
-Item B
